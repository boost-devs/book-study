<div align="center">
    <h1>4장. 훈련 데이터</h1>
    <i>moderated by <a href="https://github.com/bsm8734">샐리</a></i>
</div>

## 📝 목차

- [💬 이야기 주제](#-이야기-주제)

(⭐️ 작성 후에 목차도 추가해주세요!)

---

## 4.1. 샘플링

- 샘플링을 하면 작업을 빠르게 수행할 수 있고, 비용이 줄어든다.

### 4.1.1. 비확률 샘플링

- 데이터를 확률이 아닌 기준에 의거해 선택하는 방법
- 실데이터를 잘 대표하지 못하고 선택 편향이 강하지만, 사용이 편리하여 많이 사용된다.

1. `편의 샘플링`: 가용성에 의거해 데이터 샘플을 선택</br>(구하기 쉬운 데이터 사용, 정보 제공 동의를 한 고객 데이터만 사용)
2. `눈덩이 샘플링`: 기존 샘플을 기반으로 미래의 샘플을 선택</br>(임의로 만든 사용자 계정을 만든 후 해당 계정을 팔로우하는 계정을 모두 스크랩)
3. `판단 샘플링`: 전문가가 어떤 샘플을 포함할지 결정
4. `할당 샘플링`: 무작위화 없이 특정 데이터 그룹별 할당량에 의거해 샘플 선택</br>(설문조사 시, 실제 연령에 상관없이 각 연령 그룹마다 응답 100개씩 수집)

### 4.1.2. 단순 무작위 샘플링

- 가장 단순한 형태의 무작위 샘플링은 각 샘플이 선택될 확률이 모두 동일하다.
- 구현이 쉬우나, 드물게 발생하는 범주의 데이터가 포함되지 않을 수 있다.

### 4.1.3. 계층적 샘플링

- 모집단을 상이한 성질의 그룹으로 나눈 뒤, 각 그룹에 개별적으로 샘플링을 수행한다.
- 아무리 드물게 발생하더라도 해당 클래스의 샘플이 포함된다.
- 항상 가능하지는 않다.(ex. 어떤 샘플은 A, B 클래스에 모두 속할 수 있다.)

### 4.1.4. 가중 샘플링

- 각 샘플에 가중치가 있어, 이를 기반으로 샘플이 선택될 확률이 결정된다.
- 도메인 전문 지식을 적용할 수 있다.
- 선택 확률을 높이고 싶다면 더 높은 가중치를 부여한다.
- 샘플 가중치(ML): 가중치가 높은 샘플은 손실함수에 더 많은 영향을 주어 모델의 결정 경계를 변화시킨다.
<p align="center"><img width="400" alt="image" src="https://github.com/boost-devs/book-study/assets/35002768/883af7e0-6f10-48ca-9909-0a9d04109eca"></p>

### 4.1.5. 저수지 샘플링

- 프로덕션 환경의 스트리밍 데이터를 처리할 때 특히 유용한 알고리즘
- example
    - 지속적으로 수집되는 트윗 스트림에서 분석/훈련을 위해 k개의 트윗을 샘플링하기
    - 요구사항
        - 각 트윗이 선택될 확률은 동일
        - 알고리즘 가동을 언제든지 멈출 수 있으며 이때 각 트윗이 올바른 확률로 샘플링 됐음을 보장해야함
    - 솔루션
        - 알고리즘은 배열 형태로 구현 가능한 저장소를 포함한다.
        1. 첫 k개의 요소를 저장소에 넣는다.
        2. 수집되는 각 n번째 요소마다 1<=i<=n을 만족하는 난수 i를 생성한다.
        3. 1<=i<=k라면 저장소의 i번째 요소를 n번째 요소로 교체한다. 아니라면 다음 요소로 넘어간다.
    - 결과
        - 수집되는 각 n번째 요소가 저장소에 포함될 확률이 k/n임을 뜻한다.(각 샘플이 선택될 확률 동일)
<p align="center"><img width="600" alt="image" src="https://github.com/boost-devs/book-study/assets/35002768/57225c71-f621-44ff-82a5-7175807c2aad"></p>

### 4.1.6. 중요도 샘플링

- 원하는 분포가 아닌 다른 확률 분포만 사용 가능한 상황에서, 원하는 확률 분포에서의 샘플링이 가능하다.
- example
    - 확률분포 P에서 x를 샘플링해야하는데, P는 샘플링 비용이 크고 느리며 활용이 어렵다. </br> 반면, 확률분포 Q는 샘플링하기 훨씬 쉽다.
    - x를 Q에서 대신 샘플링하고, 해당 샘플의 가중치를 P/Q로 부여한다.
    - Q = 제안분포, 중요도 분포
- 정책 기반 강화학습(ML)
    - 새로운 정책의 가치함수를 추정하고자 할 때, 행동에 따르는 총 보상을 계산하는 일은 비용이 크다. </br> 이때, 이전 정책과 유사성이 크면, 이전 정책을 기반으로 총 보상을 계산하고 새로운 정책에 따른 가중치로 재조정하는 방법을 사용할 수 있다.



---

## 💬 이야기 주제

> <strong><i>🐧: 왜 펭귄은 귀여울까요?</i></strong>
