<div align="center">
    <h1>11장. 머신러닝의 인간적 측면</h1>
    <i>moderated by <a href="https://github.com/peacecheejecake">튜브</a></i>
</div>

## 📝 목차

- [11.1. 사용자 경험](#111-사용자-경험)
  - [11.1.1. 사용자 경험 일관성 보장하기](#1111-사용자-경험-일관성-보장하기)
  - [11.1.2. '대부분 맞는' 예측에 맞서기](#1112-대부분-맞는-예측에-맞서기)
  - [11.1.3. 원만한 실패](#1113-원만한-실패)
- [11.2. 팀 구조](#112-팀-구조)
  - [11.2.1. 크로스-펑셔널 팀 협업](#1121-크로스-펑셔널-팀-협업)
  - [11.2.2. 엔드-투-엔드 데이터 과학자](#1122-엔드-투-엔드-데이터-과학자)
- [11.3. 책임 있는 AI](#113-책임-있는-AI)
  - [11.3.1. 무책임한 AI: 사례 연구](#1131-무책임한-AI-사례-연구)
  - [11.3.2. 책임 있는 AI의 프레임워크](#1132-책임-있는-AI의-프레임워크)

---

## 11.1. 사용자 경험

ML 시스템이 좋은 사용자 경험에 제기하는 세 가지 난제와 그 해결 방안

### 11.1.1. 사용자 경험 일관성 보장하기

- ML 예측은 확률론적이며 일반적이지 않다.
- 예측 컨텍스트에 따라 같은 사용자에게도 매번 다른 예측을 할 수 있는데, 이것이 사용자에게 혼란을 줄 수 있다.
- 정확도를 일부 포기하는 방법으로 어느 정도 일관성을 보장해야 할 수도 있다. (일관성-정확성 트레이드오프)
  - Booking.com은 일정한 규칙(예: 사용자가 목적지를 변경한 경우)을 정해 조건에 해당하는 경우에만 신규 추천을 보여줌

### 11.1.2. '대부분 맞는' 예측에 맞서기

- ML 모델의 예측은 대부분 맞지만 항상 옳지는 않다.
- 대부분 맞는 예측은 예측 결과를 쉽게 수정할 수 있는 사용자(예: 고객 지원 ML 시스템의 운영자)에게 유용하다.<br/>
  하지만 그렇지 않은 경우(예: GPT가 생성한 코드로 웹사이트를 개발하는 경우)에는 그다지 유용하지 않다.
- **휴먼 인 더 루프(human-in-the-loop) AI**는 동일한 입력에 대한 여러 예측 결과를 사용자에게 제시해 사용자가 선택할 수 있게 한다.
  - 인간이 개입해 최상의 예측을 선택, 기계가 생성한 예측을 개선
  - 참고: [인간과 AI의 상호 작용에 대한 새로운 시각](https://jessylin.com/2020/06/08/rethinking-human-ai-interaction/)

### 11.1.3. 원만한 실패

- 빠른 모델이라고 하더라도 보통 특정 쿼리에는 시간이 오래 걸린다.<br/>
  (예: 언어 모델이나 시계열 모델은 데이터 길이가 길어질수록 예측이 오래 걸린다.)
- 해결 방법 1) 백업 시스템
  - 주 모델에 비해 정확도는 떨어지지만 빠른 예측이 가능한 모델을 백업 모델로 둔다.
  - 휴리스틱이나 단순 모델, 혹은 (주 모델에 의해) 미리 계산된 예측을 캐싱하는 방법 등을 채택할 수 있다.
  - 주어진 쿼리에 대한 예측을 주 모델이 생성하는 데 걸리는 시간을 예측하고, 시간이 오래 걸릴 것으로 보이는 쿼리는 백업 모델로 라우팅한다.
  - 모델이 추가됨으로써 ML 시스템의 추론 레이턴시가 증가한다.
  - 모델을 하나만 선택하는 경우 속도-정확도 드레이드오프가 있지만, 백업 시스템을 사용하면 속도가 빠른 모델과 정확도가 높은 모델을 모두 활용할 수 있다.

## 11.2. 팀 구조

최적의 ML 팀 구조

### 11.2.1. 크로스-펑셔널 팀 협업

- SME(주제 전문가 - 의사, 변호사, 은행가, 농부, 스타일리스트 등)를 ML 시스템 프로젝트 수명주기 전반에 참여시키면 많은 이점을 얻을 수 있다.
- 데이터 레이블링 및 재레이블링은 프로젝트 수명 주기 내내 진행되며, 문제 정의 및 공식화, 피처 엔지니어링, 오류 분석, 리랭킹(reranking) 예측, 사용자 인터페이스 개선 등에 큰 도움을 준다.
- SME와 엔지니어가 서로 다른 역할을 하며 협업하는 것에는 다양한 어려움이 있을 수 있다.
  - 엔지니어링이나 통계 지식이 없는 SME에게 알고리즘의 한계와 기능을 설명하는 것, 도메인 지식을 코드로 구현하는 것, 버전 관리 등
- SME가 프로젝트 계획 단계 초기에 참여하도록 하고, 엔지니어에게 권한 부여를 요청하지 않고도 프로젝트에 기여할 수 있도록 해야 한다.
- 노코드 및 로우코드 플랫폼을 구축하여 SME이 ML 시스템에 더 많이 기여하도록 도울 수 있다.

### 11.2.2. 엔드-투-엔드 데이터 과학자

- MLOps 전문성을 갖추기 위해 기업은 아래 두 접근법 중 하나를 택하는 경향이 있다.

#### 접근법 1: 별도의 팀을 구성해 프로덕션 관리하기

- 테이터 과학 및 ML 팀은 개발 환경에서 모델을 개발하고, 별도의 팀(운영, 플랫폼, ML 엔지니어링 팀)이 프로덕션 환경에 모델을 배포한다.
- 인력 고용이 보다 용이하다.
- 하지만 단점도 많다.
  - 커뮤니케이션 및 조정이 원활하지 않으면 한 팀이 다른 팀에 방해가 될 수 있다.
  - 무언가 잘못됐을 때 어느 팀의 코드가 문제인지 알기 어렵고, 디버깅을 위해 여러 팀의 협력이 필요하다.
  - 문제와 원인을 알았음에도 불구하고 다른 팀에게 책임을 미룰 수 있다.
  - 어느 누구도 전체 프로세스를 개선하는 가시성을 가지고 있지 않다.

#### 접근법 2: 데이터 과학자가 전체 프로세스를 담당하게 하기

- 데이터 과학자는 전체 프로세스의 모든 것을 알고 있는 유니콘이 된다.
- 데이터 과학자가 전체 프로세스를 담당하려면 좋은 인프라가 필요하다.
  - 데이터 과학과 인프라에 필요한 기술은 매우 다르므로 데이터 과학자가 인프라까지 정확히 이해하며 담당하기는 어렵다.
  - 데이터 과학자가 인프라에 대한 걱정 없이 프로세스를 엔드-투-엔드로 담당할 수 있는 추상화가 필요하다.
  - 넷플릭스에서는 프로젝트를 시작할 때 컨테이너화, 분산 처리, 자동 장애 조치, 기타 고급 컴퓨터 과학 전문가가 참여했다가 이를 자동화한다. 데데이터 과학자는 이러한 도구를 활용해 프로젝트를 엔드-투-엔드로 담당할 수 있다.

## 11.3. 책임 있는 AI

- 사용자에게 권한을 부여하고, 신뢰를 낳고, 사회에 공정하고 긍정적인 영향을 보장하기 위해 좋은 의도와 충분한 인식으로 AI 시스템을 설계, 개발, 배포하는 관행
- 더 이상 철학적인 것이 아니며 정책 입안자와 실무자 모두가 진지하게 고민해야 하는 사항

### 11.3.1. 무책임한 AI: 사례 연구

#### 사례 연구 1: 자동 채점기의 편향

2020년 여름, 코로나19로 인해 영국의 A-레벨 시험이 취소되었고, 오프퀄(교육 및 시험 규제 기관)은 '불공정한' 교사 평가에만 근거하는 대신 이전 이전 성적 데이터와 교사 평가를 결합하는 '알고리즘'에 근거해 A-레벨 성적을 부여하는 방식을 채택

1. 잘못된 목표 설정
   - 예측 성적을 과거 학교 간의 성적 분포와 일치하게 맞추는 것을 목표로 함
   - 과거 입시 실적이 저조한 학교에서 성적이 우수한 집단을 불균형적으로 강등
   - 학생의 현재 성적보다 학교의 과거 입시 실적을 우선시함으로써, 소외 계층 학생이 많이 재학하는 학교의 학생들에게 불이익을 줌
2. 편향을 발견하는 세분화된 모델 평가 부족
   - 일부 학교에 만연한 인종 차별, 교사의 낮은 기대치로 인한 편향은 불이익으로 이어짐
   - 과거 입시 실적에 기초했으므로 소규모 학교에 대한 데이터가 부족
3. 투명성 부족
   - 공정성 유지를 위해 모델을 비공개적으로 개발함으로써 대중이 신뢰하는 독립적인 전문가의 검토를 받지 못함

#### 사례 연구 2: 익명화된 데이터의 위험성

- 사용자 보호를 위해 개인 식별 정보(정보가 적용되는 개인의 신원을 이름, 주소, 전화번호 같은 직간접적인 수단으로 합리적으로 추론할 수 있는 정보의 표현)를 익명화
- 스트라바(온라인 피트니스 트래커)는 개인 식별 정보를 익명화했지만, 군인들의 데이터가 포함된 공개 데이터를 통해 해외 미군 기지의 활동이 드러남
  - 기본적으로 사용자의 개인 정보를 수집하도록 했고(옵트아웃), 원하지 않으면 사용자가 수동으로 선택을 해제하도록 함
  - 일부 개인 정보는 모바일 앱이 아닌 웹사이트에서만 변경할 수 있었음
- 데이터 가로채기에 대비해야 함<br/>
  스트라바에 비해 규모가 훨씬 큰 아마존, 페이스북, 구글이었다면 사회 전반에 훨씬 더 큰 위험을 초래할 수 있음

### 11.3.2. 책임 있는 AI의 프레임워크

#### 모델 편향의 출처 찾아내기

- 학습 데이터: 모델 개발에 사용한 데이터가 모델이 실제로 처리할 데이터를 대표하는지?
- 레이블링: 인간 어노테이터가 데이터를 레이블링 한다면 레이블 품질을 어떻게 측정할지? 어노테이터의 주관적 경험이 아닌 표준 지침을 따르도록 하려면 어떻게 해야 할지?
- 피처 엔지니어링: 모델이 민감한 정보가 포함된 피처를 사용하는지?
- 모델의 목표: 모든 사용자에게 공평하게 적용될 수 있는 목표로 모델을 최적화하는지?
- 평가: 다양한 사용자 그룹에 대한 모델의 성능을 이해하기 위해 적절하고 세분화된 평가를 수행하는지?

#### 데이터 기반 접근법의 한계 이해하기

- 데이터는 실제 사람들에 관한 것이며 고려해야 할 사회경제적, 문화적 측면이 있음
- ML 시스템에 영향받을 사람들의 실제 경험을 녹여낼 수 있도록 도메인 전문가와 논의해야 함

#### 서로 다른 요구 사항 간의 트레이드오프 이해하기

- 개인 정보 보호 vs. 정확도 - 차등 개인 정보 보호(differential privacy)
  - 데이터셋 내 개인에 대한 정보는 숨기면서 그룹들의 패턴을 설명함으로써 데이터셋에 대한 정보를 공유하는 시스템
  - 데이터베이스에서 임의 단일 대체의 효과가 충분히 작다면, 쿼리 결과로 개인에 대해 많은 것을 추론할 수 없다는 점에서 개인 정보를 보호
  - 다만 정확도 감소는 모든 샘플에서 동일하지는 않음
- 간결함 vs. 공정성

#### 사전 대응하기

- 비용과 시간 절약을 위해 윤리적 문제를 우회하기로 결정한다면 나중에는 결국 비용이 훨씬 높은 위험을 발견할 뿐

#### 모델 카드 생성하기

- 훈련된 ML 모델과 함께 제공되는 짧은 문서로, 모델이 어떻게 훈련되고 평가됐는지에 대한 정보를 제공
- 이해관계자가 배포를 위한 후보 모델을 비교할 때 전통적인 평가 지표뿐 아니라 윤리적, 포괄적, 공정한 고려 사항의 축을 따르도록 하믕로써 윤리적 관행과 보고를 표준화하는 것

#### 편향 완화를 위한 프로세스 수립하기

- 임시방편적인 프로세스가 아닌, 체계적인 프로세스가 필요
- 다양한 이해관계자가 접근하기 쉬운 내부 도구 포트폴리오를 만들면 좋음

#### 책임 있는 AI에 관한 최신 정보 파악하기

- 책임 있는 AI에 대한 새로운 난제가 끊임없이 등장하고 있음
- 이에 맞서기 위한 최신 연구 동향을 파악하는 것이 좋음
