<div align="center">
    <h1>6장. 모델 개발과 오프라인 평가</h1>
    <i>moderated by <a href="https://github.com/CoodingPenguin">펭귄</a></i>
</div>

## 📝 목차

- [💬 이야기 주제](#-이야기-주제)

(⭐️ 작성 후에 목차도 추가해주세요!)

---

> **모델 개발은 반복 프로세스입니다. 반복이 끝날 때다 모델 성능을 비교해봐야 하죠. 이번 반복 단계 결과물의 성능이 이전 반복 단계와 비교해서 프로덕션 환경에 얼마나 적합할지 평가합니다.**

## 6.1. 모델 개발과 훈련

### 6.1.1. 머신러닝 모델 평가

문제에 대한 모델을 선택할 때는 가용한 모델 전체가 아니라 **문제에 일반적으로 적합한 모델 집합**에 집중해야 한다.

- `예시 1` 유해한 트윗을 탐지하는 시스템 개발한다고 할 때, 해당 문제는 텍스트 분류 문제이므로 나이브 베이즈, 로지스틱 회귀, 순환 신경망, 트랜스포머 기반 모델을 사용한다.
- `예시 2` 이상 거래 탐지 시스템을 개발한다고 할 때, 전형적인 이상 탐지 문제이므로 k-최근접 이웃, 아이솔레이션 포레스트, 클러스트링, 신경망 등의 알고리즘을 사용한다.

#### 최첨단만 추종하는 함정에 빠지지 않기

- 최첨단 모델이 의미하는 바는 **"일부 정적 데이터셋에서 기존 모델보다 성능이 더 좋다"**는 뜻이다.
- 즉, 내가 가지고 있는 데이터에서는 성능이 잘 안 나올 수도 있다는 의미이다.

#### 가장 단순한 모델부터 시작하기

- 단순한 모델을 선택하면 배포하기 쉽고 복잡하지 않기 때문에 디버깅하기 수월하다.
- 단, 단순하다는 의미가 항상 노력이 최소한으로 드는 모델을 의미하는 것은 아니다.
  - 사전 훈련된 BERT 모델은 시작하기 쉬울 수 있다.
  - 하지만 성능을 개선하려고 할 때 모델 자체가 복잡하기 때문에 개선하기에 많은 노력이 든다.

#### 모델을 선택할 때 사람의 편향을 주의하기

- 모델 평가 과정에는 사람의 편향이 들어갈 수 밖에 없다.
- 그렇기 때문에 서로 다른 아키텍처를 비교할 때는 비교 가능한 설정 아래에서 비교하는 것이 좋다.
  - 예를 들어, A 모델은 실험을 100회 수행했는데 B 모델은 실험을 5회만 수행했다면 비교가 되지 않는다.

#### 현재 성과와 미래 성과를 비교 평가하기

- 현재 최적인 모델은 두 달 후에도 최적이 아닐 수 있다.
- 이 때 학습 곡선으로 데이터가 늘어남에 따라 모델 성능이 어떻게 변할지 가늠해볼 수 있다.
  - 훈련 데이터 증가에 따라 성능이 정확히 얼마나 향상될지 추정은 못 하지만 성능상 이점이 있을지 대략적으로 파악이 가능하다.

![나이브 베이즈와 SVM 모델의 학습 곡선](./img/ch06/training-curve.png)

#### 트레이드오프를 평가하기

모델을 선택할 때 수많은 **트레이드오프**를 고려해야 한다.

- `ex1` FP(False Positive) vs FN(False Negative)
  - 지문 잠금 해제의 경우 모르는 사람이 지문 잠금을 해제하는 것이 더 위험하기 때문에 FP가 적어야 한다.
  - 코로나 19 선별 검사의 경우 코로나에 걸린 사람이 아니라고 판단하는 것이 더 위험하기 떄문에 FN이 적어야 한다.
- `ex2` 연산량 요구 사항 vs 정확도
  - 복잡한 모델은 정확도가 높지만 예측값을 짧은 처리 시간 내에 내려면 CPU보다는 GPU를 사용해야 한다.
- `ex3` 해석 가능성 vs 성능
  - 복잡한 모델은 성능이 좋지만 결과를 해석하기 어렵다.

#### 모델의 가정을 이해하기

- 현실은 모델보다 훨씬 복잡하고 모델은 이를 어느 정도 근사를 할 뿐이다.
- 모든 모델은 자신만의 가정이 있으며, 이를 잘 이해하면 상황에 맞는 모델이 무엇인지 판단할 수 있다.

다음은 흔히 사용하는 가정을 나열한 것이다.

> **예측 가정**
> 입력 X에서 Y를 예측하는 것이 목표인 모델은 X를 기반으로 Y를 예측할 수 있다.

> **IDD(Independent and Identically Distributed)**
> 신경망에서 모든 데이터 포인트가 동일한 결합 분포에서 독립적으로 추출되었다고 가정한다.

> **매끄러움(Smoothness)**
> 입력 X가 출력 Y를 생성한다면 에 가까운 입력값은 비례적으로 Y에 가까운 출력값을 생성한다.

> **계산 가능성(Tractability)**
> X는 입력이고 Z는 X의 잠재 표현이라고 할 때 생성 모델은 확률 $P(Z|X)$를 계산할 수 있다.

> **경계(Boundaries)**
> 선형 분류기는 결정 경계가 선형이라고 가정한다.

> **조건부 독립(Conditional Independence)**
> 나이브 베이즈 분류기는 정해진 클래스에 대해 속성값들이 상호 독립이라고 가정한다.

> **정규 분포(Normally Distributed)**
> 많은 통계정 방법론은 데이터가 정규 분포를 따른다고 가정한다.

### 6.1.2. 앙상블

성능을 향상하기 위한 방법 중 하나로 여러 개의 모델을 앙상블하는 방법이 있다. 이 때 앙생블내의 각 모델을 기본 학습기(base learner)라고 한다.

앙상블은 배포가 복잡하고 유지 관리가 어려워 프로덕션에서 선호되지는 않지만 광고 클릭률 예측 같이 성능이 조금만 향상돼도 금전적 이ㄷ그이 큰 경우 자주 사용되가도 한다.

다음과 같이 3개의 분류기로 앙상블을 수행하여 보팅(voting)을 통해 판단한다고 할 때 얻을 수 있는 결과는 다음과 같다.

![앙상블 결과](./img/ch06/ensemble-result.png)

이 때 모든 분류기가 서로 상관관계가 없다면, 앙상블의 정확도는 **0.784(0.343 + 0.441)**이다.

#### 배깅

![배깅](./img/ch06/bagging.png)

데이터셋이 주어지만 전체 데이터셋으로 하나의 분류기를 훈련하는 대신 복원 추출을 수행해 얻은 각 데이터셋으로 모델을 훈련한다.

- 분류 문제에서는 다수결 투표로 최종 예측값을 결정하고 회귀 문제에서는 예측값의 평균을 내 결정한다.
- 배깅은 신경망, 분류 및 회귀 트리 등 **불안정성이 높은 모델의 성능을 개선**한다.
- 배깅을 사용하는 대표적인 모델로 랜덤 포레스트가 있다.

#### 부스팅

![부스팅](./img/ch06/boosting.png)

앙상블을 구성하는 각 학습기는 동일한 샘플 집합으로 학습을 수행하는데, 매 반복마다 잘못 분류된 샘플에 더 높은 가중치를 부여하며 반복 학습을 수행한다.

부스팅을 사용하는 모델의 예로 그래디언트 부스팅 머신(GVM)이 있다.


#### 스태킹

![스태킹](./img/ch06/stacking.png)

훈련 데이터로 기본 학습기를 훈련하고, 기본 학습기의 출력을 결합해 최종 예측을 수행하는 메타 학습기를 만든다. 메타 학습기에서는 다수결 투표 혹은 평균 투표를 수행해 최종 예측갑을 결정한다.

이 때 기본 학습기로 서로 다른 종류의 모델 집합을 사용할 수 있다.


### 6.1.3. 실험 추적과 버전 관리

실험 진행 상황과 결과를 추적하는 과정을 **실험 추적**이라고 하며, 나중에 재현하거나 다른 실험과 비교할 목적으로 실험의 모든 세부 정보를 기록하는 프로세스를 **버전 관리**라고 한다.

#### 실험 추적

실험 추적시 모든 추적 지표를 고려하는 것은 좋지만 대부분 살펴보지 않아도 되는 것들이다. 실험 추적 지표는 디버깅의 단초를 제공하므로 필요한 항목을 추려서 추적하는 것이 좋다.

실험 추적 지표의 예로 다음이 있다.

- 손실 곡선
- 모델 성능 지표
- 샘플, 예측값, 그라운드 트루스 레이블 쌍에 대한 로그
- 모델 훈련 속도
- 시스템 성능 지표
- 매개변수와 하이퍼파라미터

#### 버전 관리

ML 시스템은 **코드와 데이터**로 이루어진 것이므로, 코드 뿐만 아니라 데이터도 버전을 지정해야 한다. 데이터를 버전 관리하면 좋지만 다음과 같은 어려움으로 인해 실제 버전 관리를 하는 사람은 많지 않다.

- `이유 1` 데이터가 종종 코드보다 훨씬 크기 때문에 코드 버전 전략을 데이터 버전 관리에 동일하게 사용하기가 어렵다.
- `이유 2` diff를 어떻게 정의할지 명확하지 않고 의견이 분분하기 때문이다.
- `이유 3` 데이터 보호 규정(GDPR) 때문에 버전 관리가 어렵다.


### 6.1.4. 분산 훈련

#### 데이터 병렬 처리

![데이터 병럴 처리](./img/ch06/data-parallel-processing.png)

#### 모델 병렬 처리

![모델 병렬 처리](./img/ch06/model-parallel-processing.png)

### 6.1.5. 오토ML

#### 소프트 오토ML: 하이퍼파라미터 조정

#### 하드 오토ML: 아키텍처 탐색과 학습된 옵티마이저

## 6.2. 모델 오프라인 평가

### 6.2.1. 베이스라인

### 6.2.2. 평가 방법

#### 불변성 테스트

#### 방향 예상 테스트

#### 모델 보정

#### 신뢰도 측정

#### 슬라이스 기반 평가


---

## 💬 이야기 주제

> <strong><i>🐧: 왜 펭귄은 귀여울까요?</i></strong>
